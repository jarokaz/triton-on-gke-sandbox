# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

steps:
  # Clone GitHub repo
  - id: clone repo
    name: google/cloud-sdk:alpine
    entrypoint: /bin/bash
    args:
      - -c
      - |
          git clone https://github.com/jarokaz/triton-on-gke-sandbox.git /workspace/triton-on-gke-sandbox
          echo project_id=$PROJECT_ID
          echo region=$_REGION
          echo zone=$_ZONE
          echo network_name=$_NETWORK_NAME
          echo subnet_name=$_SUBNET_NAME
          echo repository_bucket_name=$_GCS_BUCKET_NAME
          echo cluster_name=$_GKE_CLUSTER_NAME
          echo triton_sa_name=$_TRITON_SA_NAME
          echo triton_namespace=$_TRITON_NAMESPACE
          echo bucket=$_TF_STATE_BUCKET
          echo prefix=$_TF_STATE_PREFIX
          echo machine_type=$_MACHINE_TYPE
          echo accelerator_type=$_ACCELERATOR_TYPE
          echo accelerator_count=$_ACCELERATOR_COUNT

  # Enable APIs
  - id: enable apis
    name: gcr.io/cloud-builders/gcloud
    entrypoint: /bin/bash
    args:
      - -c
      - |
          gcloud services enable \
            cloudbuild.googleapis.com \
            compute.googleapis.com \
            cloudresourcemanager.googleapis.com \
            iam.googleapis.com \
            container.googleapis.com \
            cloudapis.googleapis.com \
            cloudtrace.googleapis.com \
            containerregistry.googleapis.com \
            iamcredentials.googleapis.com \
            monitoring.googleapis.com \
            logging.googleapis.com \
            storage.googleapis.com \
            mesh.googleapis.com
          gcloud container fleet mesh enable --project $PROJECT_ID
    env:
      - 'PROJECT_ID=$PROJECT_ID'

  # Initialize and Apply Terraform
  - id: 'tf apply'
    name: gcr.io/cloud-builders/gcloud
    entrypoint: 'sh'
    args: 
      - '-c'
      - |
          # install Terraform
          cd /workspace
          apt update && apt install wget
          wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | tee /usr/share/keyrings/hashicorp-archive-keyring.gpg
          echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | tee /etc/apt/sources.list.d/hashicorp.list
          apt update && apt install terraform
          cd /workspace/triton-on-gke-sandbox/env-setup/terraform
          # initialize terraform
          terraform init \
            -backend-config="bucket=$_TF_STATE_BUCKET" \
            -backend-config="prefix=$_TF_STATE_PREFIX" 
          # apply terraform
          terraform apply -auto-approve \
            -var=project_id=$PROJECT_ID \
            -var=region=$_REGION \
            -var=zone=$_ZONE \
            -var=network_name=$_NETWORK_NAME \
            -var=subnet_name=$_SUBNET_NAME \
            -var=repository_bucket_name=$_GCS_BUCKET_NAME \
            -var=cluster_name=$_GKE_CLUSTER_NAME \
            -var=triton_sa_name=$_TRITON_SA_NAME \
            -var=triton_namespace=$_TRITON_NAMESPACE \
            -var=machine_type=$_MACHINE_TYPE \
            -var=accelerator_type=$_ACCELERATOR_TYPE \
            -var=accelerator_count=$_ACCELERATOR_COUNT

  # Enable automatic sidecar injection
  - id: enable sidecar injection
    name: gcr.io/cloud-builders/kubectl
    entrypoint: /bin/bash
    args:
      - -c
      - |
          gcloud container clusters get-credentials ${_GKE_CLUSTER_NAME} --project ${PROJECT_ID} --zone ${_ZONE} 
          kubectl -n istio-system get controlplanerevision
          kubectl label namespace $_TRITON_NAMESPACE  istio-injection- \
            istio.io/rev=$(kubectl -n istio-system get controlplanerevision -o=jsonpath='{.items..metadata.name}') \
            --overwrite
    env:
      - 'CLOUDSDK_COMPUTE_ZONE=$_ZONE'
      - 'CLOUDSDK_CONTAINER_CLUSTER=$_GKE_CLUSTER_NAME'

  # Install the gateway
  - id: install ingress gateway
    name: gcr.io/cloud-builders/kubectl
    entrypoint: /bin/bash
    args:
      - -c
      - |
          cd /workspace/triton-on-gke-sandbox/env-setup
          kubectl apply -n $_TRITON_NAMESPACE -f istio-ingressgateway
    env:
      - 'CLOUDSDK_COMPUTE_ZONE=$_ZONE'
      - 'CLOUDSDK_CONTAINER_CLUSTER=$_GKE_CLUSTER_NAME'

  # Deploy NVIDIA GPU drivers
  - id: deploy NVIDIA GPU drivers
    name: gcr.io/cloud-builders/kubectl
    entrypoint: /bin/bash
    args:
      - -c
      - |
          kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml 
    env:
      - 'CLOUDSDK_COMPUTE_ZONE=$_ZONE'
      - 'CLOUDSDK_CONTAINER_CLUSTER=$_GKE_CLUSTER_NAME'

  # Copy models to the repository
  - id: copy models to Triton model repo
    name: gcr.io/cloud-builders/gcloud
    entrypoint: /bin/bash
    args:
      - -c
      - |
          gcloud storage cp -r /workspace/triton-on-gke-sandbox/env-setup/model_repository gs://${_GCS_BUCKET_NAME} 

  # Configure Triton Deployment parameters & Deploy the configuration
  - id: configure and deploy Triton
    name: gcr.io/cloud-builders/kubectl
    entrypoint: /bin/bash
    args:
      - -c
      - |
          gcloud components install kustomize -q
          cd /workspace/triton-on-gke-sandbox/env-setup/kustomize
          cat << EOF > /workspace/triton-on-gke-sandbox/env-setup/kustomize/configs.env
          model_repository=gs://${_GCS_BUCKET_NAME}/model_repository
          ksa=${_TRITON_SA_NAME}
          EOF
          kustomize edit set namespace $_TRITON_NAMESPACE
          kustomize edit set  image "nvcr.io/nvidia/tritonserver:22.01-py3=nvcr.io/nvidia/tritonserver:22.01-py3"
          kubectl kustomize ./
          kubectl apply -k ./
          kubectl wait pods --for=jsonpath='{.status.phase}'=Running -n $_TRITON_NAMESPACE $(kubectl get pod --selector="app=triton-server" -n $_TRITON_NAMESPACE --output jsonpath='{.items[0].metadata.name}') --timeout=600s

    env:
      - 'CLOUDSDK_COMPUTE_ZONE=$_ZONE'
      - 'CLOUDSDK_CONTAINER_CLUSTER=$_GKE_CLUSTER_NAME'

  # Run healthcheck
  - id: run health check on Triton
    name: gcr.io/cloud-builders/kubectl
    entrypoint: /bin/bash
    args:
      - -c
      - |
          set -x
          sleep 60
          kubectl get pods -n $_TRITON_NAMESPACE
          kubectl get services -n $_TRITON_NAMESPACE
          # echo $(kubectl get services -n $_TRITON_NAMESPACE -o=jsonpath='{.items[?(@.metadata.name=="istio-ingressgateway")].spec.clusterIP}')
          echo $(kubectl get services -n $_TRITON_NAMESPACE -o=jsonpath='{.items[?(@.metadata.name=="istio-ingressgateway")].status.loadBalancer.ingress[0].ip}') | xargs -I istio_ip_addr curl -v istio_ip_addr/v2/health/ready
          # kubectl port-forward -n $_TRITON_NAMESPACE $(kubectl get pod --selector="app=triton-server" -n $_TRITON_NAMESPACE --output jsonpath='{.items[0].metadata.name}') 8000:8000 &
          # sleep 60
          # curl -s -o /dev/null -w "%{http_code}" http://localhost:8000/v2/health/ready
    env:
      - 'CLOUDSDK_COMPUTE_ZONE=$_ZONE'
      - 'CLOUDSDK_CONTAINER_CLUSTER=$_GKE_CLUSTER_NAME'

options:
  dynamic_substitutions: true